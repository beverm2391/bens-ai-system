Scraping https://arxiv.org/abs/2309.08600...
{
  "success": true,
  "data": {
    "markdown": "# Computer Science > Machine Learning\n\n**arXiv:2309.08600** (cs)\n\n\n\\[Submitted on 15 Sep 2023 ( [v1](https://arxiv.org/abs/2309.08600v1)), last revised 4 Oct 2023 (this version, v3)\\]\n\n# Title:Sparse Autoencoders Find Highly Interpretable Features in Language Models\n\nAuthors: [Hoagy Cunningham](https://arxiv.org/search/cs?searchtype=author&query=Cunningham,+H), [Aidan Ewart](https://arxiv.org/search/cs?searchtype=author&query=Ewart,+A), [Logan Riggs](https://arxiv.org/search/cs?searchtype=author&query=Riggs,+L), [Robert Huben](https://arxiv.org/search/cs?searchtype=author&query=Huben,+R), [Lee Sharkey](https://arxiv.org/search/cs?searchtype=author&query=Sharkey,+L)\n\nView a PDF of the paper titled Sparse Autoencoders Find Highly Interpretable Features in Language Models, by Hoagy Cunningham and 4 other authors\n\n[View PDF](/pdf/2309.08600)\n\n> Abstract:One of the roadblocks to a better understanding of neural networks' internals is \\\\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\\\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\\\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\n\n|     |     |\n| --- | --- |\n| Comments: | 20 pages, 18 figures, 2 tables |\n| Subjects: | Machine Learning (cs.LG); Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2309.08600](https://arxiv.org/abs/2309.08600) \\[cs.LG\\] |\n|  | (or [arXiv:2309.08600v3](https://arxiv.org/abs/2309.08600v3) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2309.08600](https://doi.org/10.48550/arXiv.2309.08600)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Robert Huben \\[ [view email](/show-email/d274585a/2309.08600)\\]\n\n**[\\[v1\\]](/abs/2309.08600v1)**\nFri, 15 Sep 2023 17:56:55 UTC (3,437 KB)\n\n**[\\[v2\\]](/abs/2309.08600v2)**\nTue, 19 Sep 2023 17:20:52 UTC (3,597 KB)\n\n**\\[v3\\]**\nWed, 4 Oct 2023 13:17:38 UTC (3,684 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Sparse Autoencoders Find Highly Interpretable Features in Language Models, by Hoagy Cunningham and 4 other authors\n\n- [View PDF](/pdf/2309.08600)\n- [TeX Source](/src/2309.08600)\n- [Other Formats](/format/2309.08600)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](/prevnext?id=2309.08600&function=prev&context=cs.LG \"previous in cs.LG (accesskey p)\")\u00a0 \\| \u00a0[next\u00a0>](/prevnext?id=2309.08600&function=next&context=cs.LG \"next in cs.LG (accesskey n)\")\n\n[new](/list/cs.LG/new) \\| [recent](/list/cs.LG/recent) \\| [2023-09](/list/cs.LG/2023-09)\n\nChange to browse by:\n\n\n[cs](/abs/2309.08600?context=cs)\n\n[cs.CL](/abs/2309.08600?context=cs.CL)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2309.08600)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2309.08600)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2309.08600)\n\n[a](/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2309.08600&description=Sparse Autoencoders Find Highly Interpretable Features in Language Models \"Bookmark on BibSonomy\") [![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2309.08600&title=Sparse Autoencoders Find Highly Interpretable Features in Language Models \"Bookmark on Reddit\")\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2309.08600) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
    "metadata": {
      "title": "[2309.08600] Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "description": "Abstract page for arXiv paper 2309.08600: Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png",
      "language": "en",
      "ogTitle": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "ogDescription": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
      "ogUrl": "https://arxiv.org/abs/2309.08600v3",
      "ogImage": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "arXiv.org",
      "viewport": "width=device-width, initial-scale=1",
      "msapplication-TileColor": "#da532c",
      "theme-color": "#ffffff",
      "og:type": "website",
      "og:site_name": "arXiv.org",
      "og:title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "og:url": "https://arxiv.org/abs/2309.08600v3",
      "og:image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "og:image:secure_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "og:image:width": "1200",
      "og:image:height": "700",
      "og:image:alt": "arXiv logo",
      "og:description": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
      "twitter:site": "@arxiv",
      "twitter:card": "summary",
      "twitter:title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "twitter:description": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts....",
      "twitter:image": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png",
      "twitter:image:alt": "arXiv logo",
      "citation_title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "citation_author": [
        "Cunningham, Hoagy",
        "Ewart, Aidan",
        "Riggs, Logan",
        "Huben, Robert",
        "Sharkey, Lee"
      ],
      "citation_date": "2023/09/15",
      "citation_online_date": "2023/10/04",
      "citation_pdf_url": "http://arxiv.org/pdf/2309.08600",
      "citation_arxiv_id": "2309.08600",
      "citation_abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
      "sourceURL": "https://arxiv.org/abs/2309.08600",
      "url": "https://arxiv.org/abs/2309.08600",
      "statusCode": 200
    }
  }
}
